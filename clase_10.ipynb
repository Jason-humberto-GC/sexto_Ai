{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - Lectura de un archivo de texto:\n",
    "\n",
    "open(): Función integrada de Python para abrir un archivo en modo de lectura (\"r\").\n",
    "\n",
    "read(): Método de los objetos de archivo en Python que lee todo el contenido del archivo y lo devuelve como una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es escribe en el archivo\n",
      "Esto tambien\n",
      "mira, puedo escribir \"comillas\" \n",
      "gracias a la diagonal invertida:  \n",
      "hola\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lectura de un archivo de texto:\n",
    "import nltk\n",
    "\n",
    "# Especifica la ruta del archivo de texto\n",
    "file_path = \"gpi.txt\"\n",
    "\n",
    "# Lee el archivo de texto\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Muestra el texto leído\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. -Tokenización:\n",
    "\n",
    "word_tokenize(): Función de NLTK que divide una cadena de texto en una lista de palabras o tokens.\n",
    "\n",
    "sent_tokenize(): Función de NLTK que divide una cadena de texto en una lista de oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras tokenizadas: ['Esto', 'es', 'escribe', 'en', 'el', 'archivo', 'Esto', 'tambien', 'mira', ',', 'puedo', 'escribir', '``', 'comillas', \"''\", 'gracias', 'a', 'la', 'diagonal', 'invertida', ':', 'hola']\n",
      "Oraciones tokenizadas: ['Esto es escribe en el archivo\\nEsto tambien\\nmira, puedo escribir \"comillas\" \\ngracias a la diagonal invertida:  \\nhola']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Tokenización de palabras\n",
    "words = word_tokenize(text)\n",
    "print(\"Palabras tokenizadas:\", words)\n",
    "\n",
    "# Tokenización de oraciones\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Oraciones tokenizadas:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. -Análisis de Concordancia:\n",
    "\n",
    "Text.concordance(): Método de los objetos Text en NLTK que muestra fragmentos de texto que contienen una palabra específica y su contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "'' gracias a la diagonal invertida : hola \n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "# Crear un objeto Text de NLTK\n",
    "text_obj = Text(words)\n",
    "\n",
    "# Buscar concordancias de una palabra específica\n",
    "word_to_search = \"hola\"\n",
    "text_obj.concordance(word_to_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. - Frecuencia de Palabras:\n",
    "\n",
    "FreqDist(): Constructor de la clase FreqDist en NLTK que calcula la frecuencia de ocurrencia de cada elemento en una lista y la devuelve como un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más comunes: [('Esto', 2), ('es', 1), ('escribe', 1), ('en', 1), ('el', 1), ('archivo', 1), ('tambien', 1), ('mira', 1), (',', 1), ('puedo', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Calcular la frecuencia de las palabras\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Imprimir las palabras más comunes y sus frecuencias\n",
    "print(\"Palabras más comunes:\", freq_dist.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. -Preprocesamiento de Texto (Eliminación de Stopwords y Stemming):\n",
    "\n",
    "stopwords.words(): Método de nltk.corpus.stopwords que devuelve una lista de palabras vacías (stopwords) en un idioma específico.\n",
    "\n",
    "PorterStemmer(): Constructor de la clase PorterStemmer en NLTK que realiza el stemming de palabras usando el algoritmo de Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras después de preprocesamiento: ['escrib', 'archivo', 'tambien', 'mira', ',', 'puedo', 'escribir', '``', 'comilla', \"''\", 'gracia', 'diagon', 'invertida', ':', 'hola']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Eliminar stopwords\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Aplicar stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "\n",
    "print(\"Palabras después de preprocesamiento:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. - Etiquetado de Palabras (POS tagging):\n",
    "\n",
    "pos_tag(): Función de NLTK que etiqueta las palabras en una lista con sus categorías gramaticales (part-of-speech tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras etiquetadas: [('I', 'PRP'), ('love', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Ejemplo de texto\n",
    "text = \"I love natural language processing.\"\n",
    "\n",
    "# Tokenización de palabras\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Etiquetar las palabras con sus categorías gramaticales\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "print(\"Palabras etiquetadas:\", tagged_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. -Análisis de Sentimientos:\n",
    "\n",
    "SentimentIntensityAnalyzer(): Constructor de la clase SentimentIntensityAnalyzer en NLTK que proporciona un analizador de sentimientos basado en reglas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuación de sentimiento: {'neg': 0.0, 'neu': 0.23, 'pos': 0.77, 'compound': 0.7717}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Ejemplo de texto\n",
    "text = \"I love natural language processing.\"\n",
    "\n",
    "# Inicializar el analizador de sentimientos\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analizar el sentimiento del texto\n",
    "sentiment_score = sia.polarity_scores(text)\n",
    "print(\"Puntuación de sentimiento:\", sentiment_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. -Búsqueda de Colocaciones:\n",
    "\n",
    "BigramCollocationFinder.from_words(): Método de la clase \n",
    "\n",
    "BigramCollocationFinder en NLTK que crea un buscador de colocaciones de bigramas a partir de una lista de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colocaciones de bigramas: [('I', 'love'), ('language', 'processing'), ('love', 'natural'), ('natural', 'language'), ('processing', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "# Encontrar colocaciones de bigramas\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "collocations = finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(\"Colocaciones de bigramas:\", collocations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. -Identificación de Entidades Nombradas (NER):\n",
    "\n",
    "ne_chunk(): Función de NLTK que identifica entidades nombradas en un texto etiquetado con POS y las agrupa en un árbol de sintaxis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades Nombradas: (S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos necesarios para el análisis\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"Barack Obama was born in Hawaii. He served as the 44th President of the United States.\"\n",
    "\n",
    "# Tokenización de palabras\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Identificar entidades nombradas\n",
    "named_entities = ne_chunk(pos_tag(words))\n",
    "print(\"Entidades Nombradas:\", named_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. -Escritura de Texto en Archivos:\n",
    "\n",
    "Puedes utilizar funciones y métodos integrados de Python para escribir texto en archivos de texto plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el texto que se escribirá en el archivo\n",
    "text_to_write = \"Este es un ejemplo de texto que se escribirá en un archivo.\"\n",
    "\n",
    "# Abrir un archivo en modo de escritura\n",
    "with open('archivo_salida.txt', 'w') as file:\n",
    "    # Escribir el texto en el archivo\n",
    "    file.write(text_to_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. -División de Texto en Párrafos:\n",
    "\n",
    "Si necesitas dividir un texto en párrafos, puedes utilizar la función nltk.tokenize.blankline_tokenize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Párrafos: ['Párrafo 1', 'Párrafo 2', 'Párrafo 3']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "\n",
    "# Texto con múltiples párrafos\n",
    "text = \"Párrafo 1\\n\\nPárrafo 2\\n\\nPárrafo 3\"\n",
    "\n",
    "# Dividir el texto en párrafos\n",
    "paragraphs = blankline_tokenize(text)\n",
    "print(\"Párrafos:\", paragraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. -Búsqueda de Expresiones Regulares en Texto:\n",
    "\n",
    "Puedes utilizar la función nltk.Text.concordance() junto con expresiones regulares para buscar y mostrar fragmentos de texto que coincidan con un patrón específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "# Ejemplo de texto con palabras de 5 letras\n",
    "text = \"El análisis de texto es una tarea fascinante para los lingüistas y científicos.\"\n",
    "\n",
    "# Crear un objeto Text de NLTK\n",
    "text_obj = Text(text.split())\n",
    "\n",
    "# Buscar y mostrar fragmentos de texto que coincidan con una expresión regular\n",
    "pattern = r'\\b[a-zA-Z]{5}\\b'  # Patrón para encontrar palabras de exactamente 5 letras\n",
    "text_obj.concordance(pattern)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
